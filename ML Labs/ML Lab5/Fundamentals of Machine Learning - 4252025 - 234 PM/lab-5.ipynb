{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3ea10c-3639-413f-94c9-f54254b6c185",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 5 : Linear Dimensionality Reduction and Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034e88a-25ea-4a4b-9776-25a5f6eebb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from palmerpenguins import load_penguins\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from plot_lib import plot_data, plot_model, set_default\n",
    "\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ebaac5-4b11-46c4-adf3-f82d2612f91b",
   "metadata": {},
   "source": [
    "## Part 1 : Linear Dimensionality Reduction - PCA, SVD and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffae8ee-8e2d-4391-88b4-78c06fe9b2a4",
   "metadata": {},
   "source": [
    "- The transformation of data from a high-dimensional space to a low-dimensional space\n",
    "- The transformed representation retains as much information about the original representation as possible\n",
    "- Useful for data visualization, cluster analysis and classification in fields such as signal processing and computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88070a-29cf-4811-b084-d2ffd204aa5b",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA): step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f85a5-9f7d-455f-82dc-e5fdcf51c643",
   "metadata": {},
   "source": [
    "Computational goal: to find **Principal Components** that \n",
    "\n",
    "\n",
    "- Are linear combinations of the original ones\n",
    "\n",
    "- Are uncorrelated with one another\n",
    "\n",
    "- Are orthogonal in original dimension space\n",
    "\n",
    "- Capture as much of the original variance in the data as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783fded-4fd0-4ccd-b968-c62c75f26e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('pca.png', width=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f9313-6367-4e57-bb0a-8ed81fe1dfc1",
   "metadata": {},
   "source": [
    "Given some data represented as an $n\\times d$ matrix $X$, where $n$ is the number of samples and $d$ is the dimension of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e5691-b1a2-4c38-ab08-06451d1c0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.load_digits()\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f45c27-f83a-4bbc-913e-d3ce94eebee0",
   "metadata": {},
   "source": [
    "Find the mean $\\mu$ ($d$-dimensional vector) of all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0404e11-5e1d-4171-a476-adaecb837e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X, axis=0)\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd03e2-54f2-48a2-aa1e-487a5293ad9f",
   "metadata": {},
   "source": [
    "Compute the covariance matrix $C = (X-\\mu)^T(X-\\mu)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b40dd4-cdac-4695-ba5d-61914bb5f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = (X - mu).T @ (X - mu)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472ab96-2533-41c9-a476-8c612f00ac0a",
   "metadata": {},
   "source": [
    "Compute the $k$ eigenvectors of $C$ (ordered by decreasing eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba865e-c194-47b5-b260-82e6034da688",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals, eigvecs = np.linalg.eig(C)\n",
    "print(eigvals.shape, eigvecs.shape, eigvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4844b-043f-4191-ac4f-d006ebee4a3b",
   "metadata": {},
   "source": [
    "Arrange such eigenvectors in a $d \\times k$ matrix $E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41722d80-95fb-4138-a116-65031807f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.copy(eigvecs)\n",
    "print(E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb18748-e13a-4712-ae97-b95fbe14d970",
   "metadata": {},
   "source": [
    "Compute the projected samples as $P = X \\cdot E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd3c15-cdda-4c24-8be6-3b2846ac38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = X @ E\n",
    "np.shape(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb367f9-58ea-4661-846e-0798f77a6b47",
   "metadata": {},
   "source": [
    "Plot the first $2$ principal components of $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35ded1-f8e3-4564-9917-998c8a40b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(mnist.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22']\n",
    "for i, c, label in zip(target_ids, colors, mnist.target_names):\n",
    "    plt.scatter(P[y == i, 0], P[y == i, 1], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9e0ec-f3e9-4c6c-94b6-0153e383ba2e",
   "metadata": {},
   "source": [
    "Compute the reconstruction as $\\tilde{X} = P \\cdot E^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbd7d9-ea22-4e4d-99c2-727b8f886a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recon = P @ E.T\n",
    "print(X_recon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e092f2-d3ab-47ed-a960-3dd81d9405d5",
   "metadata": {},
   "source": [
    "Compute the root-mean-square error (RMSE) between the actual versus reconstructed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f3fa9-40d1-492b-a195-7c1259581e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((X - X_recon)**2))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b7bf4d-1381-4f9c-918e-ad50103b33f5",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA) using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95876b9b-5418-4049-b13e-a2052555ba8c",
   "metadata": {},
   "source": [
    "Load the mnist dataset once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331af088-6166-451b-aa60-966481cd2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.load_digits()\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30933eff-b58c-4649-924e-e3e7e592e5bb",
   "metadata": {},
   "source": [
    "Fit model to data selecting the first 2 principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a69b0-551d-4eca-b467-f1688e72175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10, whiten=True)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5dda8-55f5-485c-a147-b2f905ab6e15",
   "metadata": {},
   "source": [
    "Compute the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43dcb9-5873-40a0-af30-11988463db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbd172-2eb2-4f19-994d-dc614e26c49e",
   "metadata": {},
   "source": [
    "Visualize the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500d6d8-9bd9-44f0-bb2e-240d13fba456",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(mnist.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, colors, mnist.target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=c, label=label)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef22e77-c0bb-4d75-be34-b3001dab7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdefdd8-4bcf-4079-ad3a-8379bbadd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = pca.explained_variance_ratio_\n",
    "print(f'''The explained variance of PC1 is {explained_var[0]:.2%}\n",
    "The explained variance of PC2 is {explained_var[1]:.2%}\n",
    "The total explained variance is {explained_var.sum():.2%}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768bf40-6719-452e-aae9-bfddf14b91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(pca.n_components):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(pca.components_[i].reshape(8,8), interpolation='nearest', clim=(-.15, .15), cmap='gray');\n",
    "    plt.axis('off');\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb3072-3493-4c44-bbd7-3db11c1b5eac",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d779c-f272-4f2b-862a-a0853a318f30",
   "metadata": {},
   "source": [
    "The singular value decomposition of an $m\\times n$ matrix $M$ is a factorization of the form: \n",
    "\n",
    "$$M = U {\\Sigma } V^{T} $$\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "- $U$ is an ${\\displaystyle m\\times m}$ orthogonal rotation matrix, \n",
    "\n",
    "- ${\\Sigma }$ is an ${\\displaystyle m\\times n}$ rectangular diagonal matrix of singular values, and\n",
    "\n",
    "- $V^{T}$ is an $n\\times n$ orthogonal rotation matrix.\n",
    "\n",
    "\n",
    "Use cases for the SVD:\n",
    "\n",
    "- Optimization and computational linear algebra: computing the pseudoinverse, solving homogeneous systems of linear equations, etc.\n",
    "\n",
    "- Machine learning and statistics: dimension reduction via the principal component analysis (PCA) algorithm\n",
    "\n",
    "- Applications: image compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954268e4-7910-4e23-843b-620c03234ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('svd_cartoon.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d7ef6-0b22-4b2f-9b1e-d97cbd302d27",
   "metadata": {},
   "source": [
    "The singular value decomposition can be done with the `linalg.svd()` function from NumPy (note that `np.linalg.eig(A)` works only on square matrices and will give an error for `A`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93241516-1b1b-4e54-b10c-e5425dbed8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[7, 2], [3, 4], [5, 3]])\n",
    "U, S, V = np.linalg.svd(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f0141-6748-4866-942b-83ab85e0005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78cd8aa-f675-462c-b448-c5ae83e31ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f64831-f5a2-4c4a-86c8-4f5b3792c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133778cb-1836-44d5-8c4f-04415e502d67",
   "metadata": {},
   "source": [
    "### Image Compression via the Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0b548-4d03-4c50-a1e2-aad83279d2af",
   "metadata": {},
   "source": [
    "Load image as ${\\displaystyle n\\times m}$ matrix of [RGB] values and grayscale it so we are left with ${\\displaystyle n\\times m}$ matrix of pixel intensity values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d98f2-146e-4395-a24e-fc90b992b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('fox.jpeg') # load\n",
    "imggray = img.convert('LA') # grayscale\n",
    "imgmat = np.array(list(imggray.getdata(band=0)), float) # convert to numpy array\n",
    "imgmat.shape = (imggray.size[1], imggray.size[0]) # get handle on dimensions\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(imgmat, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc44cf-baee-4b67-9aec-965785677e3c",
   "metadata": {},
   "source": [
    "Compute the SVD using NumPy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d981d3-40bd-463f-8c75-d3b7d02c6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = np.linalg.svd(imgmat)\n",
    "print(\"img: {}; U: {}; S: {}; V: {}\".format(imgmat.shape, U.shape, S.shape, V.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dbc6dd-0027-4542-ba08-6aaa41dd5dd9",
   "metadata": {},
   "source": [
    "Iterate over list of singular values and plot the reconstructed image for each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b66766-b9df-42d0-ba17-883a3d6c1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [5, 10, 25, 50, 100]:\n",
    "    reconstruct_img = np.matrix(U[:, :i]) * np.diag(S[:i]) * np.matrix(V[:i, :])\n",
    "    print(\"img: {}\\nU': {}\\nS': {}\\nV': {}\".format(imgmat.shape, U[:, :i].shape, np.diag(S[:i]).shape,V[:i, :].shape))\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(reconstruct_img, cmap='gray')\n",
    "    title = \"n = %s\" % i\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b04e5d-9598-47fb-a258-3927ae4b21c3",
   "metadata": {},
   "source": [
    "### Relation between PCA and SVD\n",
    "Simply put, the PCA viewpoint requires that one compute the eigenvalues and eigenvectors of the covariance matrix, which is the product $\\frac{1}{n}XX^T$, where $X$\n",
    "is the centered data matrix. Since the covariance matrix is symmetric, the matrix is diagonalizable, and the eigenvectors can be normalized such that they are orthonormal:\n",
    "$$ \\frac{1}{n}XX^T = \\frac{1}{n-1}WDW^T $$\n",
    "\n",
    "On the other hand, applying SVD to the data matrix $X$ as follows:\n",
    "$$ X = U\\Sigma V^T $$\n",
    "\n",
    "and attempting to construct the covariance matrix from this decomposition gives\n",
    "$$\\frac{1}{n-1} XX^T = \\frac{1}{n-1}(U \\Sigma V^T) (U \\Sigma V^T)^T = \\frac{1}{n-1}(U \\Sigma V^T)( V\\Sigma U^T)$$\n",
    "\n",
    "and since $V$ is an orthogonal matrix $(V^T V = I)$,\n",
    "$$ \\frac{1}{n-1}XX^T  = \\frac{1}{n-1} U\\Sigma^2 U^T $$\n",
    "and the correspondence is easily seen (the square roots of the eigenvalues of $XX^T$ are the singular values of $X$, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0fa1b8-2fc3-48bd-9124-5078b8b4a4ab",
   "metadata": {},
   "source": [
    "### Visualize the penguins dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd189a-d856-4433-b57a-6446344d8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = load_penguins().dropna()\n",
    "X = penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']]\n",
    "y = penguins['species']\n",
    "print(X.shape, y.shape)\n",
    "# X.head()\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "\n",
    "X -= X_mean.to_numpy()\n",
    "X /= X_std.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3153f0-5b14-4aab-968a-42831156f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = X[y=='Adelie'].to_numpy()\n",
    "class1 = X[y=='Chinstrap'].to_numpy()\n",
    "class2 = X[y=='Gentoo'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86139ce1-7819-4933-a724-97bfdd4e059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(class0[:, 0], class0[:, 1], class0[:, 2], c='blue')\n",
    "ax.scatter(class1[:, 0], class1[:, 1], class1[:, 2], c='red')\n",
    "ax.scatter(class2[:, 0], class2[:, 1], class2[:, 2], c='green')\n",
    "ax.set_xlabel('Bill Length')\n",
    "ax.set_ylabel('Bill Depth')\n",
    "ax.set_zlabel('Flipper Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ea590-e283-4cb6-8531-d39dbbad76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = np.linalg.svd(X.T)\n",
    "print(U.shape, S.shape, Vh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b97672-de70-4a27-8cdc-40758cc04e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = U[:, :]\n",
    "print(U.shape, class0.shape)\n",
    "Y_0 = U.T @ class0.T\n",
    "Y_1 = U.T @ class1.T\n",
    "Y_2 = U.T @ class2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d216a-dfc0-4748-b16f-f6fe3ee0593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(Y_0[0], Y_0[1], Y_0[2], c='blue')\n",
    "ax.scatter(Y_1[0], Y_1[1], Y_1[2], c='red')\n",
    "ax.scatter(Y_2[0], Y_2[1], Y_2[2], c='green')\n",
    "\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92768992-a3ff-4ec6-929a-3deb0c6ccf29",
   "metadata": {},
   "source": [
    "### Linear Discriminator Analysis\n",
    "While goal of PCA is to find components that maximize the variance, LDA makes use of the class labels to maximize the ratio of the between-class scatter matrix to the within-class scatter matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a496191-476e-4919-8d66-bf8df4319969",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.load_digits()\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "print(X.shape, y.shape)\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaaf06-1479-4059-870d-40b062d329d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e161e43-56b3-4a3b-b1ee-ad97d437245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, colors, mnist.target_names):\n",
    "    plt.scatter(X_lda[y==i, 0], X_lda[y==i, 1], c=c, label=label)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6311d71-1066-4250-b18e-31b875306c6d",
   "metadata": {},
   "source": [
    "### Classification using LDA\n",
    "Since LDA is a supervised learning setup, it can directly be used as a classification algorithm as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87dc41-9fa7-4ded-b1f9-467c4ad0ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading 28x28 MNIST to show classification performance.\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist['data'], mnist['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3bb58-d08e-4d3b-9efb-26a5c88eb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf0cee-9ebc-43fc-9a59-3ca585ae41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lda = lda.transform(X_test)\n",
    "y_pred = lda.predict(X_test)\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "print(\"Test set accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad54df-cb54-4d00-8560-5cb929d420bc",
   "metadata": {},
   "source": [
    "## Part 2 : Clustering - Kmeans and Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7552b2-9d17-4783-b0e8-6cc6a75afb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages from sci-kit learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import vq # Specifically uesful for K-means clustering\n",
    "from sklearn import cluster  # Clustering algorithms such as K-means and agglomerative\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans \n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.manifold import MDS #Import the multidimensional scaling module\n",
    "from scipy.spatial.distance import squareform #Import squareform, which creates a symmetric matrix from a vector\n",
    "import time\n",
    "import math\n",
    "from sklearn import mixture\n",
    "from scipy.stats import multivariate_normal as normal # Multivariate Gaussian distributions\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c414088-a5e8-4ac6-8913-5d070851d210",
   "metadata": {},
   "source": [
    "### K-means Clustering\n",
    "\n",
    "We'll start by looking at the sci-kit learn implementation of K-means for a synthetic dataset that has distinct clusters.  The cell below generates a synthetic dataset with 4 well-separated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a2a4c-d601-4cd2-9810-87067048d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the synthetic dataset.\n",
    "X1 = np.random.multivariate_normal(size = 10, mean = np.array([3, 3]), cov = np.identity(2))\n",
    "X2 = np.random.multivariate_normal(size = 10, mean = np.array([-3, 3]), cov = np.identity(2))\n",
    "X3 = np.random.multivariate_normal(size = 10, mean = np.array([-3, -3]), cov = np.identity(2))\n",
    "X4 = np.random.multivariate_normal(size = 10, mean = np.array([3, -3]), cov = np.identity(2))\n",
    "\n",
    "X = np.vstack([X1,X2,X3,X4])\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 's', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = 'o', label = 'Cluster 2')\n",
    "plt.scatter(X3[:,0], X3[:,1], c = 'g', marker = 'p', label = 'Cluster 3')\n",
    "plt.scatter(X4[:,0], X4[:,1], c = 'm', marker = '+', label = 'Cluster 4')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Synthetic dataset with 4 clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e62b-0329-483e-96eb-5714bbfbfcd9",
   "metadata": {},
   "source": [
    "### Distortion\n",
    "\n",
    "A key quantity in helping determine a good number of clusters to use is the distortion\n",
    "$$\n",
    "J = \\sum_{i=1}^N \\|{\\bf x}_i - \\mu_{C(i)}\\|^2\n",
    "$$\n",
    "where ${\\bf x}_i$ are the data points, $C(i) \\in \\{1,\\ldots,K\\}$ is the cluster assignment for ${\\bf x}_i$ and $\\mu_j$ for $j=1,\\ldots,K$ are the centers of the clusters.  Intuitively, the distortion captures the unexplained variation in the dataset after accounting for the clusters.  If $K = N$, then $\\mu_{C(i)} = {\\bf x}_i$ and the distortion will be 0.  In this case there is a cluster at every data point so intuitively there is no unexplained variation.  However, having a large number of clusters is often not very useful since we will likely be overfitting to noise in the data.  There will often be a certain point where the distortion starts to decrease more slowly.  This is called the \"elbow method\", which is what we plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd48b75-34d8-44d1-9ac1-baeb1650202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to keep track of the distortions for K=1,...,N.\n",
    "distortions = np.zeros(len(X))\n",
    "\n",
    "for k in range(1, len(X) + 1):\n",
    "    kmeans = cluster.KMeans(k, n_init=\"auto\") # K-means object in sci-kit learn with k clusters.\n",
    "    kmeans.fit(X)              # This is the line that actually runs the K-means algorithm.\n",
    "    distortions[k-1] = kmeans.inertia_ # In sci-kit learn the distortion is called the inertia.\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(np.arange(1, len(X)+1, 1), distortions, 'b-x', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Elbow Curve for K-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaea61b-6a1d-446f-ad56-05a04543ec7e",
   "metadata": {},
   "source": [
    "Indeed we see from the plot above that the distortion decreases rapidly up until $k=4$, which was the true number of clusters for our data.  After this point we begin overfitting to the noise and the distortion will not decrease as much.  A general heuristic is to choose $k$ where the kink, or elbow, in the curve occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf28b1a-2bce-46c4-ace3-a63774e1d686",
   "metadata": {},
   "source": [
    "This data was well-separated, however.  Let's see how the distortion behave whenever there is overlap between the clusters.  The cell below generates the new fake dataset with the clusters closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67d190-1daf-4794-a7b8-907051da5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the new synthetic dataset.  The only difference from before is that the means are closer together now.\n",
    "X1 = np.random.multivariate_normal(size = 10, mean = np.array([1, 1]), cov = np.identity(2))\n",
    "X2 = np.random.multivariate_normal(size = 10, mean = np.array([-1, 1]), cov = np.identity(2))\n",
    "X3 = np.random.multivariate_normal(size = 10, mean = np.array([-1, -1]), cov = np.identity(2))\n",
    "X4 = np.random.multivariate_normal(size = 10, mean = np.array([1, -1]), cov = np.identity(2))\n",
    "\n",
    "X = np.vstack([X1,X2,X3,X4])\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 's', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = 'o', label = 'Cluster 2')\n",
    "plt.scatter(X3[:,0], X3[:,1], c = 'g', marker = 'p', label = 'Cluster 3')\n",
    "plt.scatter(X4[:,0], X4[:,1], c = 'm', marker = '+', label = 'Cluster 4')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Synthetic dataset with 4 clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee176d35-a299-40cc-9ca7-b7501e9fe136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of the distortions for k=1,...,N.\n",
    "distortions = np.zeros(len(X))\n",
    "\n",
    "# Same code as before.\n",
    "for k in range(2, len(X) - 1):\n",
    "    kmeans = cluster.KMeans(k, n_init='auto')\n",
    "    kmeans.fit(X)\n",
    "    distortions[k-1] = kmeans.inertia_\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(np.arange(1, len(X)+1, 1), distortions, 'b-x', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Elbow Curve for K-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f432a8-e03c-4263-ac8b-8a6cc109912c",
   "metadata": {},
   "source": [
    "Now there is a more gradual decrease in the distortion and it is not as clear what choice of $k$ one should use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1401031-eb8c-469e-8b17-4703fa96e07e",
   "metadata": {},
   "source": [
    "### Silhoutte Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318d21d-18ef-4d5c-8132-ac32acf26fa2",
   "metadata": {},
   "source": [
    "![](Sscore.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2414208-74f8-487a-b2a5-b3a76d27f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = np.zeros(len(X))\n",
    "\n",
    "# Same code as before.\n",
    "for k in range(2, len(X) - 1):\n",
    "    kmeans = cluster.KMeans(k, n_init='auto')\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouettes[k-1] = silhouette_score(X, labels)\n",
    "    \n",
    "plt.plot(np.arange(1, len(X)+1, 1), silhouettes, 'r-o', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Silhouette Average')\n",
    "plt.title(r'Elbow Curve for K-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd141be-bf07-4e9b-bf21-8dad92ad102a",
   "metadata": {},
   "source": [
    "### Initializing the Cluster Centers \n",
    "One feature of K-means is that it is prone to becoming stuck in local minimum and is therefore sensitive to the initial cluster centers that are chosen.  The scipy implementation `kmeans2` allows for more flexibility in choosing the initial conditions so we also show it here as an alternative to sci-kit's implementation.  Now we see how the distortion changes after each iteration in K-means for different initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efdeb2-ea4a-439d-95b9-a73b9578d238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the distortion vs iteration for three different initial means.\n",
    "\n",
    "# Helper function to compute the distortion using the data X and the computed cluster centers\n",
    "# and labels for each point.\n",
    "def distortion(X, centers, labels):\n",
    "    N = X.shape[0]\n",
    "    J = 0\n",
    "    for i in range(N):\n",
    "        J += np.linalg.norm(X[i] - centers[labels[i]])**2\n",
    "    return J\n",
    "\n",
    "# Only use 10 iterations of K-means.\n",
    "max_iter = 10\n",
    "distortions_1 = np.zeros(max_iter)\n",
    "distortions_2 = np.zeros(max_iter)\n",
    "distortions_3 = np.zeros(max_iter)\n",
    "\n",
    "# 3 different initializations.\n",
    "K = 4 # 4 clusters\n",
    "np.random.seed(325) # Random seed is only chosen to emphasize difference between initializations.\n",
    "                    # This line could be removed.\n",
    "centers1, labels1 = vq.kmeans2(data = X, k = K, iter = 1, minit = '++')      # k-means++ initialization\n",
    "centers2, labels2 = vq.kmeans2(data = X, k = K, iter = 1, minit = 'random')  # points sampled from a Gaussian\n",
    "centers3, labels3 = vq.kmeans2(data = X, k = K, iter = 1, minit = 'points')  # points chosen from the dataset\n",
    "\n",
    "distortions_1[0] = distortion(X, centers1, labels1)\n",
    "distortions_2[0] = distortion(X, centers2, labels2)\n",
    "distortions_3[0] = distortion(X, centers3, labels3)\n",
    "\n",
    "for i in range(1, max_iter):\n",
    "    # Do 1 iteration of K-means using the cluster centers from the last iteration.\n",
    "    centers1, labels1 = vq.kmeans2(data = X, k = centers1, iter = 1, minit = 'matrix')\n",
    "    centers2, labels2 = vq.kmeans2(data = X, k = centers2, iter = 1, minit = 'matrix')\n",
    "    centers3, labels3 = vq.kmeans2(data = X, k = centers3, iter = 1, minit = 'matrix')\n",
    "    \n",
    "    distortions_1[i] = distortion(X, centers1, labels1)\n",
    "    distortions_2[i] = distortion(X, centers2, labels2)\n",
    "    distortions_3[i] = distortion(X, centers3, labels3)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_1, 'r-s', label = '++')\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_2, 'b-o', label = 'random')\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_3, 'm-x', label = 'points')\n",
    "plt.xlabel(r'Iteration')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Convergence of K-means for different initializations')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a3bd9e-f24a-4914-80a6-d0b52b4ffd6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The fact that all three curves have leveled-off indicates the K-means has converged.  However, the distortion is different meaning we have congevered to different local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11807270-e99e-4bfb-b026-9397b441bec1",
   "metadata": {},
   "source": [
    "### K-Mediods Clustering <a class=\"anchor\" id=\"second\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6b6a0-801b-415e-a6ab-a6967fd09680",
   "metadata": {},
   "source": [
    "The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups). K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers ( medoids or exemplars).\n",
    "\n",
    "K-medoids is also a partitioning technique of clustering that clusters the data set of n objects into k clusters with k known a priori. A useful tool for determining k is the silhouette.\n",
    "\n",
    "It could be more robust to noise and outliers as compared to k-means because it minimizes a sum of general pairwise dissimilarities instead of a sum of squared Euclidean distances. The possible choice of the dissimilarity function is very rich but in our applet we used the Euclidean distance.\n",
    "\n",
    "A medoid of a finite dataset is a data point from this set, whose average dissimilarity to all the data points is minimal i.e. it is the most centrally located point in the set.\n",
    "\n",
    "The most common realisation of k-medoid clustering is the Partitioning Around Medoids (PAM) algorithm and is as follows:\n",
    "\n",
    "    * Initialize: randomly select k of the n data points as the medoids\n",
    "    * Assignment step: Associate each data point to the closest medoid.\n",
    "    * Update step: For each medoid m and each data point o associated to m swap m and o and compute the total cost of the configuration (that is, the average dissimilarity of o to all the data points associated to m). Select the mediod o with the lowest cost of the configuration.\n",
    "\n",
    "Repeat alternating steps 2 and 3 until there is no change in the assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89968f-1647-4137-91f9-c32d11b12fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define distance metric\n",
    "euclidean = lambda x1,x2: np.sqrt(np.sum((x1-x2)**2,-1))\n",
    "manhattan = lambda x1,x2: np.sum(np.abs(x1-x2), -1)\n",
    "hamming = lambda x1,x2: np.sum((x1!=x2),-1)\n",
    "dist_dict={\"euclidean\":euclidean, \"manhattan\":manhattan, \"hamming\":hamming}\n",
    "class KMedoids:\n",
    "    \n",
    "    def __init__(self, K=5, max_iters=100, dist_fn=euclidean, repeats=10):\n",
    "        self.K = K\n",
    "        self.max_iters = max_iters                        #for computing each medoid\n",
    "        self.dist_fn = dist_fn\n",
    "        self.repeats = repeats                            #for several runs to compute medoids\n",
    "    \n",
    "    def fit(self, x):\n",
    "        #note that medoids stores a list of K indices \n",
    "        n,d = x.shape\n",
    "        distances = self.dist_fn(x[None,:,:], x[:,None,:])       #distance function for pairwise distance [n, n]\n",
    "        best_cost = np.inf\n",
    "        for r in range(self.repeats):\n",
    "        #we repeat the process of finding medoids for self.repeats time\n",
    "            medoids = np.random.choice(n, self.K, replace=False)      #randomly choose a list of k distinct indices from 0 to n-1\n",
    "            for t in range(self.max_iters):\n",
    "                membership = np.argmin(distances[medoids,:], axis=0)       #assign membership based on distance from the medoids\n",
    "                new_medoids = medoids.copy()\n",
    "                cost = 0\n",
    "                for i in range(self.K):\n",
    "                    cluster_inds = np.nonzero(membership == i)[0]                                    #returns the indices of points with membership i\n",
    "                    cluster_dist = np.sum(distances[np.ix_(cluster_inds, cluster_inds)], axis=1)     #pairwise distance between points with membership i and summed over axis 1  \n",
    "                    cost += np.min(cluster_dist)                                                     #compute the cost  for the \n",
    "                    new_medoids[i] = cluster_inds[np.argmin(cluster_dist)]                           #find the index of i-th medoids\n",
    "                if np.allclose(new_medoids, medoids):\n",
    "                    #print(f'converged after {t} iterations with the cost {cost}')\n",
    "                    break\n",
    "                medoids = new_medoids\n",
    "            if cost < best_cost:\n",
    "                best_medoids = medoids\n",
    "                best_membership = membership\n",
    "        return best_medoids, best_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63040792-a873-4050-bbb7-d6d6858cde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-Mediods Visualization\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = datasets.make_blobs(\n",
    "    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n",
    ")\n",
    "kmediods = KMedoids(K=3)\n",
    "mediods, labels = kmediods.fit(X)\n",
    "unique_labels = set(labels)\n",
    "fig = plt.figure(figsize =(12, 10))\n",
    "colors = [\n",
    "    plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))\n",
    "]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "plt.plot(\n",
    "    X[mediods][:, 0],\n",
    "    X[mediods][:, 1],\n",
    "    \"o\",\n",
    "    markerfacecolor=\"cyan\",\n",
    "    markeredgecolor=\"k\",\n",
    "    markersize=6,\n",
    ")\n",
    "\n",
    "plt.title(\"KMedoids clustering. Medoids are represented in cyan.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd09ca-1804-4604-936d-4d008bd1908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the representative digits that we get based on the dissimilarity measure used.\n",
    "x_org, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "def plot_digits(data):\n",
    "    num_plots = data.shape[0]\n",
    "    fig = plt.figure(figsize=(num_plots, 10.*num_plots))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(1, num_plots), axes_pad=0.1)\n",
    "    for i in range(num_plots):\n",
    "        grid[i].imshow(data[i].reshape((28,28)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150333f2-8985-4acc-8bae-ec536f3549c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_org.values[:1000]\n",
    "for distance in dist_dict.keys():\n",
    "    kmedoid = KMedoids(10, dist_fn=dist_dict[distance], repeats=1000)\n",
    "    centers, _ = kmedoid.fit(x.reshape(-1, 784))\n",
    "    print(\"Distance Function used: \", distance)\n",
    "    plot_digits(x[centers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d113776-8247-46cd-a537-dd365a856804",
   "metadata": {},
   "source": [
    "### Why to use KMediods over KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95552c22-3319-41a5-9ae4-39a483937660",
   "metadata": {},
   "source": [
    "Pros:\n",
    "1. _K-medoid is more flexible_: First of all, you can use k-medoids with any similarity measure. K-means however, may fail to converge - it really must only be used with distances that are consistent with the mean. So e.g. Absolute Pearson Correlation must not be used with k-means, but it works well with k-medoids.\n",
    "\n",
    "2. _Robustness of medoid_: Secondly, the medoid as used by k-medoids is roughly comparable to the median (in fact, there also is k-medians, which is like K-means but for Manhattan distance). If you look up literature on the median, you will see plenty of explanations and examples why the median is more robust to outliers than the arithmetic mean. Essentially, these explanations and examples will also hold for the medoid. It is a more robust estimate of a representative point than the mean as used in k-means.\n",
    "\n",
    "Cons:\n",
    "1. _k-medoids is much more expensive_: That's the main drawback. Usually, PAM takes much longer to run than k-means. As it involves computing all pairwise distances, it is $\\mathcal{O}(n^2*k*i)$; whereas k-means runs in $\\mathcal{O}(n*k*i)$ where usually, k times the number of iterations is $k*i << n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed42307-e795-4636-b3a3-4da31229a3e8",
   "metadata": {},
   "source": [
    "### Gaussian mixture models and the EM algorithm\n",
    "\n",
    "One disadvantage of K-means is that the clusters are restricted to be spherical and so it is heavily dependent on the scaling of the features.  A Gaussian Mixture Model (GMMs) has an extra covariance parameter which allows us to represent poorly-scaled (i.e. long ellipses) data. In addition, GMMs also allow for soft clustering where any point could be contributing to multiple clusters. \n",
    "\n",
    "Consider the following toy dataset generated from the Gaussian mixture model with parameters\n",
    "$$\n",
    "\\phi = (0.5,\\ 0.5),\\quad \\mu_0 = \\begin{bmatrix}-3\\\\0\\end{bmatrix},\\quad \\mu_1 = \\begin{bmatrix}3\\\\0\\end{bmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\Sigma_0 = \\Sigma_1 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 100\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The dataset is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda397a-f737-4b21-b548-a8ab5f92f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "mu1 = np.array([-3, 0])\n",
    "mu2 = np.array([3, 0])\n",
    "Sigma = np.array([[1, 0], [0, 100]])\n",
    "X1 = normal.rvs(mean = mu1, cov = Sigma, size = 25)\n",
    "X2 = normal.rvs(mean = mu2, cov = Sigma, size = 25)\n",
    "\n",
    "# Merge the data together.\n",
    "X = np.vstack([X1, X2])\n",
    "\n",
    "# Plot the data.\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 'o', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = '+', label = 'Cluster 2')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Toy Dataset from GMM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031a534-2f60-45e8-8b7e-fa4069c8681e",
   "metadata": {},
   "source": [
    "Now we'll fit a Gaussian mixture model to this data and plot the contours for the covariance as well as the locations of the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31b241-5d89-4d2f-9d1c-b2dedb32a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gaussian mixture model object with 2 components.  The 'tied' argument refers to the fact that we\n",
    "# specify the covariances of both components to be the same.  This constrains the problem and reduces the \n",
    "# number of parameters we need to estimate.  We also provide the initial parameters which we happen to know already.\n",
    "gmm = mixture.GaussianMixture(n_components = 2, covariance_type = 'tied', tol=1e-3, \\\n",
    "                              means_init = [mu1, mu2], precisions_init = np.linalg.inv(Sigma))\n",
    "\n",
    "# The fit function uses the EM algorithm.\n",
    "gmm.fit(X)\n",
    "\n",
    "# Get the fitted means and covariances.\n",
    "phi = gmm.weights_\n",
    "mu = gmm.means_\n",
    "cov = gmm.covariances_\n",
    "\n",
    "# Can also get the number of iterations that was needed for convergence.\n",
    "print(\"{:d} iterations for EM to converge.\".format(gmm.n_iter_))\n",
    "\n",
    "# Make a contour plot of the data.\n",
    "xx = np.linspace(-6, 6, 100)\n",
    "yy = np.linspace(-20, 20, 100)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "points = np.asarray([np.ravel(XX), np.ravel(YY)]).T\n",
    "Z = phi[0]*normal.pdf(points, mean = mu[0], cov = cov) + phi[1]*normal.pdf(points, mean = mu[1], cov = cov)\n",
    "ZZ = Z.reshape(XX.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contour(XX, YY, ZZ)\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 'o', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = '+', label = 'Cluster 2')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Toy Dataset from GMM fitted with EM algorithm')\n",
    "plt.legend()\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "print(mu1, mu2, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17465754-cb0e-4691-881f-73cc03a98c91",
   "metadata": {},
   "source": [
    "## Part 3 : Other clustering algorithms\n",
    "\n",
    "Both K-means and Gaussian mixture models can only fit elliptical clusters.  Sometimes data may have distinct clusters which do not fit this shape.  The following example is taken from the Sci-kit learn documentation [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62467908-23cb-4f47-8979-cfb3c1794ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph # kNN is needed for average linkage clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3,\n",
    "                'min_samples': 20,\n",
    "                'xi': 0.05,\n",
    "                'min_cluster_size': 0.1}\n",
    "\n",
    "# Parameters for the datasets.\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2,\n",
    "                     'min_samples': 20, 'xi': 0.25}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2,\n",
    "              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2,\n",
    "             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "\n",
    "    # K-Means algorithm.\n",
    "    two_means = cluster.KMeans(n_clusters=params['n_clusters'])\n",
    "\n",
    "    # Average linkage agglomerative clustering.\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", metric=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "\n",
    "    # Gaussian mixture model with EM algorithm.\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('KMeans', two_means),\n",
    "        ('GaussianMixture', gmm),\n",
    "        ('AgglomerativeClustering', average_linkage)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
