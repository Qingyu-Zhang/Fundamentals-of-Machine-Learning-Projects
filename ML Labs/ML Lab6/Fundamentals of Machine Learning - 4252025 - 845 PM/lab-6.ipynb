{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d3c0fa-b4af-4963-aaeb-37cd4c5b4279",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning (CSCI-UA.473)\n",
    "## Lab 6: Non-linear Dimensionality Reduction and Autoencoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d664a7-214b-4eee-83b0-02dcfa373873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages from sci-kit learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import vq # Specifically uesful for K-means clustering\n",
    "from sklearn import cluster  # Clustering algorithms such as K-means and agglomerative\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.utils import resample\n",
    "from scipy.spatial.distance import squareform #Import squareform, which creates a symmetric matrix from a vector\n",
    "from palmerpenguins import load_penguins\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn_som.som import SOM\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import time\n",
    "import math\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '0' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d169c-96da-48b5-bfa9-7e490d57207f",
   "metadata": {},
   "source": [
    "## Part 1 : Non Linear Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca5e34-4d9e-4fa5-981e-a96f72707cdc",
   "metadata": {},
   "source": [
    "### Loss measures when dealing with probabilities\n",
    "When dealing with probabilistic values, we have typically used the log-loss or the cross-entropy loss. These are defined as : <br />\n",
    " $L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b478c-88ba-4662-bf29-b008adbf0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babae801-334f-4218-829d-17335c57a6e3",
   "metadata": {},
   "source": [
    "The Kullback-Leibler Divergence score, or KL divergence score, quantifies how much one probability distribution differs from another probability distribution. The intuition for the KL divergence score is that when the probability for an event from P is large, but the probability for the same event in Q is small, there is a large divergence. When the probability from P is small and the probability from Q is large, there is also a large divergence, but not as large as the first case.\n",
    "It is given by : <br />\n",
    "\n",
    "$KL(P || Q) =  \\sum_{x \\in X} P(x) * \\log(\\frac{P(x)}{Q(x)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340871b-ad63-4ba0-9367-a4eb63173727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define distributions\n",
    "events = ['red', 'green', 'blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "\n",
    "print('P=%.3f Q=%.3f' % (sum(p), sum(q)))\n",
    "# plot first distribution\n",
    "plt.subplot(2,1,1).set_title('P')\n",
    "plt.bar(events, p)\n",
    "# plot second distribution\n",
    "plt.subplot(2,1,2).set_title('Q')\n",
    "plt.bar(events, q)\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9b5ad-8c33-4887-bcee-77b3f9ca0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * math.log2(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31dff31-cbf3-40c1-8748-0748f186f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate (P || Q)\n",
    "kl_pq = kl_divergence(p, q)\n",
    "print('KL(P || Q): %.3f bits' % kl_pq)\n",
    "# calculate (Q || P)\n",
    "kl_qp = kl_divergence(q, p)\n",
    "print('KL(Q || P): %.3f bits' % kl_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d673f8f-6193-4ebf-a0ce-c71166622bee",
   "metadata": {},
   "source": [
    "#### Now the algorithms,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efc037-a92d-47d5-8846-cdc9e5ec3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_digits(as_frame=True)\n",
    "X,y = df.data, df.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97944f1f-0a86-4150-a50c-98df9a06b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1bf39-ff3e-40ad-9847-7cb3c8186461",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49579f1b-c2e3-46dd-a65d-d2e9a822f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(df.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, colors, df.target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=c, label=label)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88edf9ab-1109-4d18-8ecd-312d918f0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6258420-3614-4e6d-bdf3-4671170b3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, colors, df.target_names):\n",
    "    plt.scatter(X_lda[y==i, 0], X_lda[y==i, 1], c=c, label=label)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42c6f5-e0db-469f-87dd-c9353c7267a2",
   "metadata": {},
   "source": [
    "### T-distributed Stochastic Neighbor Embedding (t-SNE) using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5eb4dc-d388-44de-977d-6dba3b1a8583",
   "metadata": {},
   "source": [
    "A statistical method for visualizing high-dimensional data by giving each datapoint a location in either a two or three-dimensional space which can be easily visualized using a scatter plot. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fe470-b7ec-4daa-a569-6b4513a33404",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2, perplexity=5, n_jobs=-1).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141be33-a752-4a4d-afcd-221c6b3acf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(df.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, colors, df.target_names):\n",
    "    plt.scatter(X_embedded[y == i, 0], X_embedded[y == i, 1], c=c, label=label)\n",
    "\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b79e83-bdd7-4172-b5d8-d38ff903495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms\n",
    "X_embedded = TSNE(n_components=2, perplexity=50, n_jobs=-1, init='pca').fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f1342-8c5d-4ed3-ac7c-71ba3ed87b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(df.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, colors, df.target_names):\n",
    "    plt.scatter(X_embedded[y == i, 0], X_embedded[y == i, 1], c=c, label=label)\n",
    "\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907a8b0-5495-4558-bfb1-13daba225c62",
   "metadata": {},
   "source": [
    "### UMAP : Uniform Manifold Approximation and Projection for Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6f206-8fe3-493a-906c-bbaba2cacc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap-learn library is designed to be similar to scikit-learn\n",
    "umap_model = UMAP(n_neighbors=50,min_dist=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f298d0b-bffc-4d9d-85ce-3d461af711e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model.fit(X)\n",
    "X_umap = umap_model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a8808-470d-4f0c-841f-8a4b359af357",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(df.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, colors, df.target_names):\n",
    "    plt.scatter(X_umap[y == i, 0], X_umap[y == i, 1], c=c, label=label)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a30a8d-67c5-49ef-983d-2d5ec7fdb6af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multi-Dimensional Scaling \n",
    "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
    "\n",
    "In general, MDS is a technique used for analyzing similarity or dissimilarity data. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e980636-8bf9-43b7-85bf-bbd019e36dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, normalized_stress='auto', dissimilarity='euclidean')\n",
    "X_mds = mds.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a127f-203f-46b5-94bc-85dc87ab6cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(df.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, colors, df.target_names):\n",
    "    plt.scatter(X_mds[y == i, 0], X_mds[y == i, 1], c=c, label=label)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Same as PCA if we use 'euclidean' as distance to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6408b90c-434d-4a2b-b641-8231286bb8a0",
   "metadata": {},
   "source": [
    "Let's load a custom dataset based on academic disciplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1365af-54f1-4212-9b13-2a0e2f4783dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('fieldsMDS.csv',delimiter=',') # load file as data\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652392f-03e8-46b8-9079-035fe13e728d",
   "metadata": {},
   "source": [
    "* Turn the matrix into a vector of pairwise distances\n",
    "* Remove all nans to leave only pairwise distances remaining\n",
    "* Create the distance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929a7c8-51ae-4c64-9bd6-32800ff0e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVec = np.ndarray.flatten(data) \n",
    "dataVec = dataVec[~np.isnan(dataVec)] \n",
    "D = squareform(dataVec) \n",
    "fieldNames = ['Math', 'Physics', 'Chemistry', 'Biology', 'Psych', 'Neuro', 'Econ', 'Sociology', 'DS', 'CS']; #These are the academic fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552ef1d-0bb9-482e-b169-25e24c0d934e",
   "metadata": {},
   "source": [
    "#### MDS Hyperparameters\n",
    "n_components: Looking for a 2D solution, n_init: Number of runs with random initial starting positions, max_iter: Max number of iterations per run, dissimilarity: We already did it, from out distance matrix, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b81587-62fa-4844-aff7-73e8e477f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, n_init=100, max_iter = 10000, dissimilarity='precomputed', normalized_stress='auto') #Create the mds object\n",
    "mdsSolution = mds.fit_transform(D) #Actually run the mds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c4688-9f9f-4e19-a00e-bbe62e8b906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mdsSolution[:,0], mdsSolution[:,1], color='blue') #Making the plot, first 2 dimensions\n",
    "for ii in range(len(mdsSolution)):\n",
    "    plt.text(mdsSolution[ii,0], mdsSolution[ii,1],fieldNames[ii])\n",
    "\n",
    "plt.xlabel('MDS axis 1')\n",
    "plt.ylabel('MDS axis 2')\n",
    "plt.show()\n",
    "\n",
    "print(mds.stress_) #How low could we get the stress? [Un-normalized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973f551-a2ac-416d-b6db-923148cf70d2",
   "metadata": {},
   "source": [
    "## Comparing PCA, t-SNE, MDS and UMAP runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf9f4a-2ed2-452c-8764-e27dff815f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_size_scaling(algorithm, data, sizes=[100, 200, 400 ,800, 1600], n_runs=5):\n",
    "    result = []\n",
    "    for size in sizes:\n",
    "        for run in range(n_runs):\n",
    "            subsample = resample(data, n_samples=size)\n",
    "            start_time = time.time()\n",
    "            algorithm.fit(subsample)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            del subsample\n",
    "            result.append((size, elapsed_time))\n",
    "    return pd.DataFrame(result, columns=('dataset size', 'runtime (s)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28e9ea-c5e0-422d-9049-4a530aaca152",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_algorithms = [\n",
    "    PCA(),\n",
    "    UMAP(),\n",
    "    TSNE(),\n",
    "    MDS(normalized_stress='auto'),\n",
    "]\n",
    "performance_data = {}\n",
    "for algorithm in all_algorithms:\n",
    "    \n",
    "    alg_name = str(algorithm).split('(')[0]\n",
    "    performance_data[alg_name] = data_size_scaling(algorithm, X, n_runs=5)\n",
    "\n",
    "    print(f\"[{time.asctime(time.localtime())}] Completed {alg_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c13a2-49c6-464d-9f7b-217acd326e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for alg_name, perf_data in performance_data.items():\n",
    "    # print(perf_data['dataset size'], perf_data['runtime (s)'])\n",
    "    plt.plot(perf_data['dataset size'], perf_data['runtime (s)'], label=alg_name)\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Time to fit (s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23966f26-253e-42b1-87a7-6b7a9f146241",
   "metadata": {},
   "source": [
    "Interactive plot to visualize UMAP results : https://grantcuster.github.io/umap-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ed8d7-ce4c-4445-8769-eb4761a13f41",
   "metadata": {},
   "source": [
    "### Density-based spatial clustering of applications with noise (DBSCAN) <a class=\"anchor\" id=\"third\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb0d73a-0d6d-46c6-9f11-6b238c9dc4fc",
   "metadata": {},
   "source": [
    "DBSCAN requires two parameters: ε (eps) and the minimum number of points required to form a dense region (minPts). It starts with an arbitrary starting point that has not been visited. This point's ε-neighborhood is retrieved, and if it contains sufficiently many points, a cluster is started. Otherwise, the point is labeled as noise. Note that this point might later be found in a sufficiently sized ε-environment of a different point and hence be made part of a cluster.\n",
    "\n",
    "If a point is found to be a dense part of a cluster, its ε-neighborhood is also part of that cluster. Hence, all points that are found within the ε-neighborhood are added, as is their own ε-neighborhood when they are also dense. This process continues until the density-connected cluster is completely found. Then, a new unvisited point is retrieved and processed, leading to the discovery of a future cluster or noise.\n",
    "\n",
    "DBSCAN can be used with any distance function (as well as similarity functions or other predicates). The distance function (dist) can therefore be seen as an additional parameter.\n",
    "\n",
    "The algorithm can be expressed in pseudocode as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a62e5-353e-4422-988a-6cf20c859ca3",
   "metadata": {},
   "source": [
    "```\n",
    "DBSCAN(DB, distFunc, eps, minPts) {\n",
    "    C := 0                                                  /* Cluster counter */\n",
    "    for each point P in database DB {\n",
    "        if label(P) ≠ undefined then continue               /* Previously processed in inner loop */\n",
    "        Neighbors N := RangeQuery(DB, distFunc, P, eps)     /* Find neighbors */\n",
    "        if |N| < minPts then {                              /* Density check */\n",
    "            label(P) := Noise                               /* Label as Noise */\n",
    "            continue\n",
    "        }\n",
    "        C := C + 1                                          /* next cluster label */\n",
    "        label(P) := C                                       /* Label initial point */\n",
    "        SeedSet S := N \\ {P}                                /* Neighbors to expand */\n",
    "        for each point Q in S {                             /* Process every seed point Q */\n",
    "            if label(Q) = Noise then label(Q) := C          /* Change Noise to border point */\n",
    "            if label(Q) ≠ undefined then continue           /* Previously processed (e.g., border point) */\n",
    "            label(Q) := C                                   /* Label neighbor */\n",
    "            Neighbors N := RangeQuery(DB, distFunc, Q, eps) /* Find neighbors */\n",
    "            if |N| ≥ minPts then {                          /* Density check (if Q is a core point) */\n",
    "                S := S ∪ N                                  /* Add new neighbors to seed set */\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "where RangeQuery is:\n",
    "\n",
    "```\n",
    "RangeQuery(DB, distFunc, Q, eps) {\n",
    "    Neighbors N := empty list\n",
    "    for each point P in database DB {                      /* Scan all points in the database */\n",
    "        if distFunc(Q, P) ≤ eps then {                     /* Compute distance and check epsilon */\n",
    "            N := N ∪ {P}                                   /* Add to result */\n",
    "        }\n",
    "    }\n",
    "    return N\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a90650-7346-467b-892e-76869a8ce23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "\n",
    "crater = pd.read_csv('crater.csv')\n",
    "assert len (crater) == 1500\n",
    "assert set (crater.columns) == set (['x_1', 'x_2', 'kmeans_label'])\n",
    "\n",
    "with open ('crater_counts.txt', 'rt') as fp:\n",
    "    true_counts = [int (c) for c in fp.read ().split (',')]\n",
    "    assert sum (crater['kmeans_label'] == 0) == true_counts[0]\n",
    "    assert sum (crater['kmeans_label'] == 1) == true_counts[1]\n",
    "\n",
    "def make_scatter_plot (df, x=\"x_1\", y=\"x_2\", hue=\"label\",\n",
    "                       palette={0: \"red\", 1: \"olive\", 2: \"blue\", 3: \"green\"},\n",
    "                       size=5,\n",
    "                       centers=None):\n",
    "    if (hue is not None) and (hue in df.columns):\n",
    "        sns.lmplot (x=x, y=y, hue=hue, data=df, palette=palette,\n",
    "                    fit_reg=False)\n",
    "    else:\n",
    "        sns.lmplot (x=x, y=y, data=df, fit_reg=False)\n",
    "\n",
    "    if centers is not None:\n",
    "        plt.scatter (centers[:,0], centers[:,1],\n",
    "                     marker=u'*', s=500,\n",
    "                     c=[palette[0]])\n",
    "\n",
    "def make_scatter_plot2 (df, x=\"x_1\", y=\"x_2\", hue=\"label\", size=5):\n",
    "    if (hue is not None) and (hue in df.columns):\n",
    "        sns.lmplot (x=x, y=y, hue=hue, data=df,\n",
    "                    fit_reg=False)\n",
    "    else:\n",
    "        sns.lmplot (x=x, y=y, data=df, fit_reg=False)\n",
    "\n",
    "make_scatter_plot (crater, hue='kmeans_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df076c3f-64d8-4342-81a7-8dae0f0300af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN Algorithm\n",
    "def region_query (p, eps, X):\n",
    "    # These lines check that the inputs `p` and `X` have\n",
    "    # the right shape.\n",
    "    _, dim = X.shape\n",
    "    assert (p.shape == (dim,)) or (p.shape == (1, dim)) or (p.shape == (dim, 1))\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    return np.linalg.norm (p - X, axis=1) <= eps\n",
    "    ### END SOLUTION\n",
    "\n",
    "def index_set (y):\n",
    "    \"\"\"\n",
    "    Given a boolean vector, this function returns\n",
    "    the indices of all True elements.\n",
    "    \"\"\"\n",
    "    assert len (y.shape) == 1\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    return set (np.where (y)[0])\n",
    "    ### END SOLUTION\n",
    "\n",
    "def find_neighbors (eps, X):\n",
    "    m, d = X.shape\n",
    "    neighbors = [] # Empty list to start\n",
    "    ### BEGIN SOLUTION\n",
    "    for i in range (len (X)):\n",
    "        n_i = index_set (region_query (X[i, :], eps, X))\n",
    "        neighbors.append (n_i)\n",
    "    ### END SOLUTION\n",
    "    assert len (neighbors) == m\n",
    "    return neighbors\n",
    "\n",
    "def find_core_points (s, neighbors):\n",
    "    assert type (neighbors) is list\n",
    "    assert all ([type (n) is set for n in neighbors])\n",
    "    \n",
    "    core_set = set ()\n",
    "    ### BEGIN SOLUTION\n",
    "    for i, n_i in enumerate (neighbors):\n",
    "        if len (n_i) >= s:\n",
    "            core_set.add (i)\n",
    "    ### END SOLUTION\n",
    "    return core_set\n",
    "\n",
    "def expand_cluster (p, neighbors, core_set, visited, assignment):\n",
    "    # Assume the caller performs Steps 1 and 2 of the procedure.\n",
    "    # That means 'p' must be a core point that is part of a cluster.\n",
    "    assert (p in core_set) and (p in visited) and (p in assignment)\n",
    "    \n",
    "    reachable = set (neighbors[p])  # Step 3\n",
    "    while reachable:\n",
    "        q = reachable.pop () # Step 4\n",
    "        \n",
    "        # Put your reordered and correctly indented statements here:\n",
    "        ### BEGIN SOLUTION\n",
    "        if q not in visited:\n",
    "            visited.add (q) # Mark q as visited\n",
    "            if q in core_set:\n",
    "                reachable |= neighbors[q]\n",
    "        if q not in assignment:\n",
    "            assignment[q] = assignment[p]\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    # This procedure does not return anything\n",
    "    # except via updates to `visited` and\n",
    "    # `assignment`.\n",
    "    \n",
    "def dbscan (eps, s, X):\n",
    "    clusters = []\n",
    "    point_to_cluster = {}\n",
    "    \n",
    "    neighbors = find_neighbors (eps, X)\n",
    "    core_set = find_core_points (s, neighbors)\n",
    "    \n",
    "    assignment = {}\n",
    "    next_cluster_id = 0\n",
    "\n",
    "    visited = set ()\n",
    "    for i in core_set: # for each core point i\n",
    "        if i not in visited:\n",
    "            visited.add (i) # Mark i as visited\n",
    "            assignment[i] = next_cluster_id\n",
    "            expand_cluster (i, neighbors, core_set,\n",
    "                            visited, assignment)\n",
    "            next_cluster_id += 1\n",
    "\n",
    "    return assignment, core_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22357541-1152-4eee-b61b-20ac813abe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization\n",
    "X = crater[['x_1', 'x_2']].values\n",
    "assignment, core_set = dbscan (0.73, 50, X)\n",
    "\n",
    "print (\"Number of core points:\", len (core_set))\n",
    "print (\"Number of clusters:\", max (assignment.values ()))\n",
    "print (\"Number of unclassified points:\", len (X) - len (assignment))\n",
    "\n",
    "def plot_labels (df, labels):\n",
    "    df_labeled = df.copy ()\n",
    "    df_labeled['label'] = labels\n",
    "    make_scatter_plot2 (df_labeled)\n",
    "\n",
    "labels = [-1] * len (X)\n",
    "for i, c in assignment.items ():\n",
    "    labels[i] = c\n",
    "plot_labels (crater, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08206f-e1e1-4eea-9df1-5bb397cd68c7",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering <a class=\"anchor\" id=\"fifth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8fc40-18f5-4645-87e7-bdb1dbd899d2",
   "metadata": {},
   "source": [
    "Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. For example, all files and folders on the hard disk are organized in a hierarchy. There are two types of hierarchical clustering, Divisive and Agglomerative.\n",
    "\n",
    "**Divisive method**\n",
    "\n",
    "In this method we assign all of the observations to a single cluster and then partition the cluster to two least similar clusters. Finally, we proceed recursively on each cluster until there is one cluster for each observation.\n",
    "\n",
    "**Agglomerative method**\n",
    "\t\t\n",
    "In this method we assign each observation to its own cluster. Then, compute the similarity (e.g., distance) between each of the clusters and join the two most similar clusters. Finally, repeat steps 2 and 3 until there is only a single cluster left.\n",
    "\n",
    "#### Linkage or distance matrix\n",
    "\n",
    "Before any clustering is performed, it is required to determine the proximity matrix containing the distance between each point using a distance function. Then, the matrix is updated to display the distance between each cluster. The following three methods differ in how the distance between each cluster is measured.\n",
    "\n",
    "**Single Linkage** \t\t\n",
    "In single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two closest points.\n",
    "<img src=http://www.saedsayad.com/images/Clustering_single.png>\n",
    "\n",
    "**Complete Linkage**\t\t\n",
    "In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points.\n",
    "<img src=http://www.saedsayad.com/images/Clustering_complete.png>\n",
    "\n",
    "**Average Linkage**\t\n",
    "In average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters “r” and “s” to the left is equal to the average length each arrow between connecting the points of one cluster to the other.\n",
    "<img src=http://www.saedsayad.com/images/Clustering_average.png>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0cb24-3ca9-4080-9cf8-b21e7bac6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Mall_Customers.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955f2e1-8a78-4747-bdc3-0f1d0090599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bcd231-b585-4741-9859-e2565e020587",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Annual income distribution\",fontsize=16)\n",
    "plt.xlabel (\"Annual income (k$)\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.hist(df['Annual Income (k$)'],color='orange',edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77faf8ca-61ba-4348-8250-0672e3da5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Spending Score distribution\",fontsize=16)\n",
    "plt.xlabel (\"Spending Score (1-100)\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.hist(df['Spending Score (1-100)'],color='green',edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895bdf45-cdc9-4e14-b034-54522b7efaf8",
   "metadata": {},
   "source": [
    "### So, is there a definitive correlation between annual income and spending score? - *Apparently not*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5782a-2ebf-44bf-aaa2-9e898839fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Annual Income and Spending Score correlation\",fontsize=18)\n",
    "plt.xlabel (\"Annual Income (k$)\",fontsize=14)\n",
    "plt.ylabel (\"Spending Score (1-100)\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.scatter(df['Annual Income (k$)'],df['Spending Score (1-100)'],color='red',edgecolor='k',alpha=0.6, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab684a4-09d6-40e1-bfbe-23e6386c975e",
   "metadata": {},
   "source": [
    "### How about correlation between age and spending score? - *Apparently not*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f2490-3447-43c7-8009-e66f841b44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Age and Spending Score correlation\",fontsize=18)\n",
    "plt.xlabel (\"Age\",fontsize=14)\n",
    "plt.ylabel (\"Spending Score (1-100)\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.scatter(df['Age'],df['Spending Score (1-100)'],color='blue',edgecolor='k',alpha=0.6, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93502a49-57e6-4e1a-80bd-4f71863c01bb",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "** Therefore, we will explore cluserting the customers based on their annual income and spending score to see if there are distinguisbale clusters which the mall can target **\n",
    "\n",
    "We could use k-means but we don't have any idea about the number of hidden clusters. We will see that hierarchial clustering with dendograms will give us a good insight on the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179b393-2715-4a29-a238-7cb84c77bfa6",
   "metadata": {},
   "source": [
    "## Dendograms\n",
    "[Dendograms](https://en.wikipedia.org/wiki/Dendrogram) are tree diagrams frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering. The clades are arranged according to how similar (or dissimilar) they are. Clades that are close to the same height are similar to each other; clades with different heights are dissimilar — the greater the difference in height, the more dissimilarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32119048-50be-4c9b-aae1-c6dd4e4032c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,[3,4]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25fe7a-631a-45bd-9eea-753e002fab3e",
   "metadata": {},
   "source": [
    "### _Ward_ distance matrix\n",
    "We will use 'Ward' distance matrix for this dendogram.\n",
    "$$d(u,v) = \\sqrt{\\frac{|v|+|s|}{T}d(v,s)^2+ \\frac{|v|+|t|}{T}d(v,t)^2- \\frac{|v|}{T}d(s,t)^2}$$\n",
    "\n",
    "where **$u$** is the newly joined cluster consisting of clusters **$s$** and **$t$**, **$v$** is an unused cluster in the forest, **$T=|v|+|s|+|t|$**, and **$|*|$** is the cardinality of its argument. This is also known as the incremental algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f61d5f-25b5-4a7e-bf30-e0106faff17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "#plt.grid(True)\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f969b1b-6a66-47a9-9fd4-ddd0c0529469",
   "metadata": {},
   "source": [
    "### Optimal number of clusters\n",
    "\n",
    "Often, the optimal number of clusters can be found from a Dendogram is a simple manner.\n",
    "* Look for the longest stretch of vertical line which is not crossed by any ***extended*** horizontal lines (here *extended* means horizontal lines i.e. the cluster dividers are extended infinitely to both directions).\n",
    "* Now take any point on that stretch of line and draw an imaginary horizontal line.\n",
    "* Count how many vertical lines this imaginary lines crossed.\n",
    "* That is likely to be the optimal number of clusters.\n",
    "\n",
    "**The idea is shown in the following figure. Here the optimal number of clusters could be 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282ad04-d517-4ea9-af58-421bdf741d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.hlines(y=190,xmin=0,xmax=2000,lw=3,linestyles='--')\n",
    "plt.text(x=900,y=220,s='Horizontal line crossing 5 vertical lines',fontsize=20)\n",
    "#plt.grid(True)\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af2c04-5d25-4035-8640-568a559a0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters = 5, metric = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9a2e9-836d-4a7e-92fc-68be65292f35",
   "metadata": {},
   "source": [
    "### Plot the clusters and label customer types\n",
    "* _Careful_ - high income but low spenders\n",
    "* _Standard_ - middle income and middle spenders\n",
    "* **_Target group_ - middle-to-high income and high spenders (should be targeted by the mall)**\n",
    "* _Careless_ - low income but high spenders (should be avoided because of possible credit risk)\n",
    "* _Sensible_ - low income and low spenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cba6f3-2770-4999-9aeb-8333d3520e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careful')\n",
    "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Standard')\n",
    "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Target group')\n",
    "plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'orange', label = 'Careless')\n",
    "plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible')\n",
    "plt.title('Clustering of customers',fontsize=20)\n",
    "plt.xlabel('Annual Income (k$)',fontsize=16)\n",
    "plt.ylabel('Spending Score (1-100)',fontsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.axhspan(ymin=60,ymax=100,xmin=0.4,xmax=0.96,alpha=0.3,color='yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eff8c2-5db7-4f99-bbe3-a15fee3d151c",
   "metadata": {},
   "source": [
    "## Verifying the optimal number of clusters by k-means algorithm\n",
    "\n",
    "Given a set of observations $(x_1, x_2, …, x_n)$, where each observation is a d-dimensional real vector, [**k-means clustering**](https://en.wikipedia.org/wiki/K-means_clustering) aims to partition the *$n$* observations into *$k$* (≤ *$n$*) sets $S = {S_1, S_2, …, S_k}$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\n",
    "\n",
    "$${\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i}}$$\n",
    "\n",
    "where $\\mu_i$ is the mean of points in $S_i$\n",
    "\n",
    "We run k-means++ model (k-means with carefully initialized centroids) iterating over number of clusters (1 to 15) and plot the ***within-cluster-sum-of-squares (WCSS) matric*** to determine the optimum number of cluster by elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466b62f-bb0c-4a80-9f4d-cdc42c67265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 16):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "with plt.style.context(('fivethirtyeight')):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1, 16), wcss)\n",
    "    plt.title('The Elbow Method with k-means++\\n',fontsize=25)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.ylabel('WCSS (within-cluster sums of squares)')\n",
    "    plt.vlines(x=5,ymin=0,ymax=250000,linestyles='--')\n",
    "    plt.text(x=5.5,y=110000,s='5 clusters seem optimal choice \\nfrom the elbow position',\n",
    "             fontsize=25,fontdict={'family':'Times New Roman'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75724ac-ecc0-4e58-8e45-c284be04f4e1",
   "metadata": {},
   "source": [
    "## Part 2:  Autoencoders\n",
    "Neural networks can be used to implement autoencoders. We will use PyTorch to implement an auto encoder for the MNIST dataset. First, as always we load the dataset and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6964ca-f237-412a-a401-d09f9d983215",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 15, 10\n",
    "  \n",
    "# Initializing the transform for the dataset\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "  \n",
    "# Downloading the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./MNIST/train\", train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True)\n",
    "  \n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./MNIST/test\", train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True)\n",
    "  \n",
    "# Creating Dataloaders from the\n",
    "# training and testing dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1024)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1024)\n",
    "  \n",
    "# Printing 25 random images from the training dataset\n",
    "random_samples = np.random.randint(\n",
    "    1, len(train_dataset), (25))\n",
    "  \n",
    "for idx in range(random_samples.shape[0]):\n",
    "    plt.subplot(5, 5, idx + 1)\n",
    "    plt.imshow(train_dataset[idx][0][0].numpy(), cmap='gray')\n",
    "    plt.title(train_dataset[idx][1])\n",
    "    plt.axis('off')\n",
    "  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8e584-2b58-405f-8a6e-170a0f0329a2",
   "metadata": {},
   "source": [
    "Next we define the models for the encoder and decoder parts of the network. Any neural network architecture can be use to implement either part. Since the MNIST is a relatively simple dataset, we will use simple fully connected deep networks, however a convolutional neural network might be preferred for richer images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e133d1a-409b-41ee-bdd4-d68b8fa2d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28 * 28, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 10)\n",
    "        )\n",
    "          \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 28 * 28),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "model = DeepAutoencoder()\n",
    "criterion = torch.nn.MSELoss()\n",
    "num_epochs = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b413150-cb41-4b96-a392-f7ac8e081983",
   "metadata": {},
   "source": [
    "We now define our training process, which iterates over the entire datasets for 100 epochs. Each step of the loop\n",
    "iterates over each batch and calculates loss between the predicted image and the original image.\n",
    "We also store images and their outputs for each epoch.\n",
    "After the loop ends, we plot out the training loss to better understand the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1137ef-99f0-49d9-a12a-a3e1c4bd5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List that will store the training loss\n",
    "train_loss = []\n",
    "  \n",
    "# Dictionary that will store the\n",
    "# different images and outputs for \n",
    "# various epochs\n",
    "outputs = {}\n",
    "  \n",
    "batch_size = len(train_loader)\n",
    "  \n",
    "# Training loop starts\n",
    "for epoch in range(num_epochs):\n",
    "        \n",
    "    # Initializing variable for storing \n",
    "    # loss\n",
    "    running_loss = 0\n",
    "    stime= time.time()\n",
    "    # Iterating over the training dataset\n",
    "    for batch in train_loader:\n",
    "            \n",
    "        # Loading image(s) and\n",
    "        # reshaping it into a 1-d vector\n",
    "        img, _ = batch  \n",
    "        img = img.reshape(-1, 28*28)\n",
    "          \n",
    "        # Generating output\n",
    "        out = model(img)\n",
    "          \n",
    "        # Calculating loss\n",
    "        loss = criterion(out, img)\n",
    "          \n",
    "        # Updating weights according\n",
    "        # to the calculated loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "        # Incrementing loss\n",
    "        running_loss += loss.item()\n",
    "      \n",
    "    # Averaging out loss over entire batch\n",
    "    running_loss /= batch_size\n",
    "    train_loss.append(running_loss)\n",
    "    print(f\"Epoch : {epoch}, loss : {running_loss}, time taken : {time.time()-stime}\")\n",
    "      \n",
    "    # Storing useful images and\n",
    "    # reconstructed outputs for the last batch\n",
    "    outputs[epoch+1] = {'img': img, 'out': out}\n",
    "  \n",
    "  \n",
    "# Plotting the training loss\n",
    "plt.plot(range(1,num_epochs+1),train_loss)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adf63d-9189-4995-8354-83eabf606e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "  \n",
    "epochs_list = [1, 5, 10, 50, 100]\n",
    "  \n",
    "# Iterating over specified epochs\n",
    "for val in epochs_list:\n",
    "    \n",
    "      # Extracting recorded information\n",
    "    temp = outputs[val]['out'].detach().numpy()\n",
    "    title_text = f\"Epoch = {val}\"\n",
    "      \n",
    "    # Plotting first five images of the last batch\n",
    "    for idx in range(5):\n",
    "        plt.subplot(7, 5, counter)\n",
    "        plt.title(title_text)\n",
    "        plt.imshow(temp[idx].reshape(28,28), cmap= 'gray')\n",
    "        plt.axis('off')\n",
    "          \n",
    "        # Incrementing the subplot counter\n",
    "        counter+=1\n",
    "    \n",
    "# Iterating over first five\n",
    "# images of the last batch\n",
    "for idx in range(5):\n",
    "      \n",
    "    # Obtaining image from the dictionary\n",
    "    val = outputs[10]['img']\n",
    "      \n",
    "    # Plotting image\n",
    "    plt.subplot(7,5,counter)\n",
    "    plt.imshow(val[idx].reshape(28, 28),\n",
    "               cmap = 'gray')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "      \n",
    "    # Incrementing subplot counter\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c794830-227a-4389-b638-cad589d607ef",
   "metadata": {},
   "source": [
    "## Kohonen Self-Organinzing Maps - Using the sklearn_som package\n",
    "The Self-Organising Map (SOM) is an unsupervised machine learning algorithm introduced by Teuvo Kohonen in the 1980s [1]. As the name suggests, the map organises itself without any instruction from others. It is a brain-inspired model. A different area of the cerebral cortex in our brain is responsible for specific activities. A sensory input like vision, hearing, smell, and taste is mapped to neurons of a corresponding cortex area via synapses in a self-organising way. It is also known that the neurons with similar output are in proximity. SOM is trained through a competitive neural network, a single-layer feed-forward network that resembles these brain mechanisms.\n",
    "The SOM algorithm can be broken down into the following steps :\n",
    "1. <b>Initialisation</b> :\n",
    "   Weights of neurons in the map layer are initialised.\n",
    "\n",
    "2. <b>Competitive process</b> :\n",
    "   Select one input sample and search the best matching unit (BMU) among all neurons in n x m grid using distance measures.\n",
    "\n",
    "3. <b>Cooperative process</b> :\n",
    "   Find the proximity neurons of BMU by neighbourhood function.\n",
    "\n",
    "4. <b>Adaptation process</b> :\n",
    "   Update the BMU and neighbours' weights by shifting the values towards the input pattern.\n",
    "   If the maximum count of training iteration is reached, exit. If not, increment the iteration count by 1 and repeat the process from 2.\n",
    "   \n",
    "Of course, the package abstracts out most of this and allows us to use it like other sklearn modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c97d2-4b68-4120-aec1-6f3a66cd5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = load_penguins().dropna()\n",
    "data = penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']].to_numpy()\n",
    "penguins['label'] = pd.factorize(penguins['species'])[0] + 1\n",
    "labels = penguins['label']\n",
    "\n",
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d246e6f-3e1c-40a6-ad68-4566f986780a",
   "metadata": {},
   "source": [
    "Now, just like with any classifier right from sklearn, we will have to build an SOM instance and call .fit() on our data to fit the SOM. We already know that there are 3 classes in the Penguins Dataset, so we will use a 3 by 1 structure for our self organizing map, but in practice you may have to try different structures to find what works best for your data. Let's build and fit the som:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc743b-7048-4862-a87a-bb725563e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "som = SOM(m=3, n=1, dim=data.shape[1], lr=0.1)\n",
    "som.fit(data, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28c430-9997-4e0a-a03a-4b8c923c799c",
   "metadata": {},
   "source": [
    "Note that when building the instance of SOM, we specify m and n to get an m by n matrix of neurons in the self organizing map.\n",
    "Also similar to sklearn, let's assign each datapoint to a predicted cluster using the .predict() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3041b-f927-4097-957a-1cc12867fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = som.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065cb4b-0b43-453b-8ddb-e191c7a419b3",
   "metadata": {},
   "source": [
    "Lets plot our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99dad2b-0321-4cef-8d66-b62056d424dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data, columns=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'])\n",
    "df['ClassLabel'] = labels\n",
    "\n",
    "# Plot using seaborn pairplot\n",
    "sns.pairplot(df, hue='ClassLabel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00607c-8154-428b-b5ca-d360bd10b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'])\n",
    "df['ClassLabel'] = predictions\n",
    "\n",
    "# Plot using seaborn pairplot\n",
    "sns.pairplot(df, hue='ClassLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f0842-7b04-44d6-bb04-587b364d2600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
