{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513512cb-d03d-421d-80b2-fd3f1a81e96e",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 4: Pytorch and Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9fcb15-be08-4bbf-9b8f-975f637240e9",
   "metadata": {},
   "source": [
    "### Perceptrons using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9c4f8-5e8d-427f-988c-ed4df0d2c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from palmerpenguins import load_penguins\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This function returns a pandas dataframe by default (use return_X_y to get it in two numpy arrays)\n",
    "penguins = load_penguins().dropna()\n",
    "X = penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']]\n",
    "y = penguins['species']\n",
    "print(X.shape, y.shape)\n",
    "X.head(), y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e94f66-179c-4dd8-a2df-a90b3b27286c",
   "metadata": {},
   "source": [
    "### Split the data into train and test\n",
    "\n",
    "We'll use a 80/20 split for our training/test sets. We will not touch the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb155363-965a-42ff-b182-f7c4dc90a633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data.  DO NOT TOUCH THE TEST DATA FROM HERE ON!!\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, test_size = 0.2) # 0.2 is 20% test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f62133-607a-4b92-8625-ab1079067273",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(tol=1e-3, random_state=0, shuffle=True)\n",
    "### This is equivalent to SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\", penalty=None)\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "X_train_norm = (X_train - mean) / std\n",
    "clf.fit(X_train_norm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5abee-ba4e-465b-9079-875258a001b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score((X_test- mean) / std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf62e49-e8d8-4414-ad40-3430401017a5",
   "metadata": {},
   "source": [
    "\n",
    "## What is PyTorch?\n",
    "\n",
    "Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "-  Tensorial library that uses the power of GPUs\n",
    "-  A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "In this class though we will not use any GPU based computation since most of the work (labs and assignments) will be done on your laptops. \n",
    "\n",
    "## Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca3353-f86d-4aa0-b514-3d251cd0bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # <Ctrl> / <Shift> + <Return>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba66dc-902f-4e7f-b27a-3d8f35950ebf",
   "metadata": {},
   "source": [
    "## The Torch Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d6c25-2abf-463c-8c96-8727bfaf77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a tensor of size 2x3x4\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701510a-22c3-4571-8d2d-3e7571a81925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the tensor\n",
    "print(t.size(), t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b017a57-6629-454e-9879-a7ff9bdf4e59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectors (1D Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669fefce-8f40-4300-bfe1-078bc815d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a 1D tensor of integers 1 to 4\n",
    "v = torch.Tensor([1, 2, 3, 4])\n",
    "\n",
    "# Create a 2x4 tensor\n",
    "m = torch.Tensor([[2, 5, 3, 7],\n",
    "                  [4, 2, 1, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee55ced-9106-49fa-b315-0a1103fb3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of dimensions (1D) and size of tensor\n",
    "print(f'dim: {v.dim()}, size: {v.size()}')\n",
    "\n",
    "# Print number of dimensions (2D) and size of tensor\n",
    "print(f'dim: {m.dim()}, size: {m.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911e0be-faf5-4b21-ba22-8a1832b62f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise multiplication\n",
    "v * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe412d-cbac-4208-9023-802b1e87f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar product: 1*1 + 2*0 + 3*2 + 4*0\n",
    "m @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58faba49-a078-4985-8f4b-fe744fff1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-place replacement of random number from 0 to 10\n",
    "x = torch.Tensor(5).random_(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e4c89-e250-4d17-b2d9-1aaa43f2f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'first: {x[0]}, last: {x[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697d150-6499-4b8c-bda3-08e6cdf056b5",
   "metadata": {},
   "source": [
    "### Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd45ffa-b9c4-4990-a075-178f8b094054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor from 3 to 8, with each having a space of 1\n",
    "torch.arange(3., 8 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde0f2b-b829-46b2-8949-b684bf9ac027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor from 5.7 to -2.1 with each having a space of -3\n",
    "torch.arange(5.7, -2.1, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6ac4a-822f-4aa0-a4df-4b2c39470f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a 1D tensor of steps equally spaced points between start=3, end=8 and steps=20\n",
    "torch.linspace(3, 8, 20).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597972e3-6fcd-4285-b7a4-dc8ca4e2f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor filled with 0's\n",
    "torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17de4f5-2b85-4a11-8dad-70845f03358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor filled with 1's\n",
    "torch.ones(3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a7be7-ff92-48b6-8051-6b2d9f9a3206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with the diagonal filled with 1\n",
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa2dbf-204f-4b39-b07c-8eed4d0e23c8",
   "metadata": {},
   "source": [
    "### Defining a model in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39573da0-46df-45cb-97d8-37541f039d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b930667-36d3-4ca3-9312-c631847932c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture hyperparameters\n",
    "D = 4  # input dimensions\n",
    "C = 3  # num_classes\n",
    "H = 10  # num_hidden_units\n",
    "\n",
    "# Define training hyperparameters\n",
    "learning_rate = 1e-2\n",
    "lambda_l2 = 1e-1 # coefficient for the L2 regularizer. You should play with its value to see the effect of regularization\n",
    "\n",
    "# nn package to create our linear model. Notice the Sequential container class. \n",
    "# Each Linear module has a weight and bias\n",
    "# The order in which the Linear modules are defined is important as it creates the directed acyclic graph\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be2788-f630-413a-94d3-51254961b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48cbad-06cd-4046-af43-9b755219a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess our data and form tensors before we can use a PyTorch Model\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "\n",
    "# First move features present in pandas dataframes to tensors, this is straightforward\n",
    "X_train_pth = torch.as_tensor(X_train.values, dtype=torch.float)\n",
    "X_test_pth = torch.as_tensor(X_test.values, dtype=torch.float)\n",
    "\n",
    "feature_means = torch.mean(X_train_pth, dim = 0)\n",
    "feature_stds = torch.std(X_train_pth, dim=0)\n",
    "print(feature_means.shape, feature_stds.shape)\n",
    "\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# X_train_pth = torch.tensor(scaler.fit_transform(X_train_pth), dtype=torch.float32)\n",
    "\n",
    "X_train_pth = (X_train_pth - feature_means)/feature_stds\n",
    "\n",
    "# Convert the labels need a little more care since Pytorch loss functions do not expect string labels\n",
    "labels = y_train.unique() # Get all unique labels from our training set\n",
    "le = preprocessing.LabelEncoder() # Define encoder using sklearn's LabelEncoder\n",
    "targets = le.fit_transform(y_train) # Transform string targets to integers\n",
    "# targets: array([0, 1, 2, 3, 4])\n",
    "\n",
    "y_train_pth = torch.as_tensor(targets).long() # Finally put everything into a tensor\n",
    "\n",
    "# Repeat for test labels\n",
    "targets = le.fit_transform(y_test) # Transform string targets to integers\n",
    "\n",
    "y_test_pth = torch.as_tensor(targets).long()\n",
    "\n",
    "print(X_train_pth.shape,y_train_pth.shape, X_test_pth.shape, y_test_pth.shape)\n",
    "print(torch.mean(X_train_pth,dim=0), torch.std(X_train_pth,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1a9f9-c343-435a-9588-579c9eb24717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package has a variety of loss functions already implemented\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# nn package also has a variety of optimization algorithms implemented\n",
    "# we use the stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass over the model to get the logits \n",
    "    y_pred = model(X_train_pth) #1\n",
    "    # print(y_pred.shape, y_train_pth.shape)\n",
    "\n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y_train_pth) #2\n",
    "    \n",
    "    # print(y_pred, y_train_pth)\n",
    "    # break\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y_train_pth == predicted).sum() / len(y_train)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # reset (zero) the gradients before running the backward pass over the model\n",
    "    # we need to do this because the gradients get accumulated at the same place across iterations\n",
    "    optimizer.zero_grad() #3\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params (weights and biases)\n",
    "    loss.backward() #4\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step() #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae9e1d-a950-473a-8c2c-5dcad7baa1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "X_test_pth = (X_test_pth - feature_means)/feature_stds\n",
    "out = model(X_test_pth)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "#get accuration\n",
    "print('Accuracy of the network %.4f %%' % (100 * torch.sum(y_test_pth==predicted).double() / len(y_test_pth)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868218b-1113-4fbc-865f-f63581ec5add",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification of non-linearly separable dataset using multi-layer perceptron\n",
    "We will not demonstrate how to build a linear model and a neural network model using PyTorch and how to train it to classify a toy dataset which is not linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a4b0f-0779-4a70-97e4-ed3908cf7bef",
   "metadata": {},
   "source": [
    "Import appropriate packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7219b8f-30f6-4a98-9727-33c52387e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some predefined helper functions provided for plotting data and model outputs\n",
    "from plot_lib import plot_data, plot_model, set_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c8061-9d19-4834-9481-2d8a46b7e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0a1f9-c605-4cb0-986b-44a662d86e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 2  # dimensions\n",
    "C = 3  # num_classes\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24239b73-a4a4-48df-8fc2-6788abdb439d",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "We will now create a data set consisting of three classes and is in the shape of a spiral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b620ce-ae8f-47b6-bc71-ef8e09516197",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X = torch.zeros(N * C, D).to(device)\n",
    "y = torch.zeros(N * C, dtype=torch.long).to(device)\n",
    "for c in range(C):\n",
    "    index = 0\n",
    "    t = torch.linspace(0, 1, N)\n",
    "    # When c = 0 and t = 0: start of linspace\n",
    "    # When c = 0 and t = 1: end of linpace\n",
    "    # This inner_var is for the formula inside sin() and cos() like sin(inner_var) and cos(inner_Var)\n",
    "    inner_var = torch.linspace(\n",
    "        # When t = 0\n",
    "        (2 * math.pi / C) * (c),\n",
    "        # When t = 1\n",
    "        (2 * math.pi / C) * (2 + c),\n",
    "        N\n",
    "    ) + torch.randn(N) * 0.2\n",
    "    \n",
    "    for ix in range(N * c, N * (c + 1)):\n",
    "        X[ix] = t[index] * torch.FloatTensor((\n",
    "            math.sin(inner_var[index]), math.cos(inner_var[index])\n",
    "        ))\n",
    "        y[ix] = c\n",
    "        index += 1\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd96cb-030d-4317-b46d-aa5c8a1cbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the data using the plot_data function provided as a helper function\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8b471b-6b62-4fdc-84d3-68477ce0d8f4",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "We now define a linear classification model using PyTorch and train it using stochastic gradient descent with the help of the autograd package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd5db0-4c4e-4f0b-8a51-9715ef3c88ad",
   "metadata": {},
   "source": [
    "Initialize some hyper-parameter values, such as, learning rate, regularization coefficient etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b38ca-d79d-4ae4-a7de-5505e58f41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "lambda_l2 = 1e-3 # coefficient for the L2 regularizer. You should play with its value to see the effect of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616eb1a-86c4-4cf8-9009-e73671472dfb",
   "metadata": {},
   "source": [
    "Create the linear model and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3c815-a008-406d-8b8e-fdfeedf21d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model. Notice the Sequential container class. \n",
    "# Each Linear module has a weight and bias\n",
    "# The order in which the Linear modules are defined is important as it creates the directed acyclic graph\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H,H),\n",
    "    nn.Linear(H, C)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652a189-ac9d-4919-b1e6-795c90550c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e63f0-f421-4536-b870-c8840209e711",
   "metadata": {},
   "source": [
    "Create the training loop and print the metrics (loss and accuracy) at the end of each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49a491-463a-481a-9a50-c93d2e98d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package has a variety of loss functions already implemented\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# nn package also has a variety of optimization algorithms implemented\n",
    "# we use the stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass over the model to get the logits \n",
    "    y_pred = model(X) #1\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y) #2\n",
    "    # score, predicted = torch.max(y_pred, 1)\n",
    "    # acc = (y == predicted).sum().float() / len(y)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # reset (zero) the gradients before running the backward pass over the model\n",
    "    # we need to do this because the gradients get accumulated at the same place across iterations\n",
    "    optimizer.zero_grad() #3\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params (weights and biases)\n",
    "    loss.backward() #4\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step() #5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555a3d5-0de3-48b2-9217-2dee55a5b5d3",
   "metadata": {},
   "source": [
    "Plot the output of the model (in this case a collection of hyper-planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb625a57-2eab-413f-9ca0-4b0da6b0a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544dd22e-6d93-4a94-b1eb-f192fc59241d",
   "metadata": {},
   "source": [
    "### Two layer neural network\n",
    "We now define a two layer (single hidden layer) neural network model using PyTorch and train it using stochastic gradient descent with the help of the autograd package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310dc17-5064-4b94-9b78-4dbd10d1c470",
   "metadata": {},
   "source": [
    "Initialize the hyper-parameters like before. Play around with the learning rate and the regularization parameter to see their effect on the optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b162d4-40c2-4904-ae90-302939432f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "lambda_l2 = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab902841-def3-4a9a-9f56-163bc382a436",
   "metadata": {},
   "source": [
    "Create and print the two layer MLP with ReLU as the activation units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28903146-6e9b-4f09-886c-611f3554215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H,C)\n",
    "    \n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c62d89-a3b5-48df-90cf-b3e3962071e8",
   "metadata": {},
   "source": [
    "Create the training loop and print the metrics (loss and accuracy) at the end of each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52806f-9a81-4f8f-86fd-fd7b7d55c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package has a variety of loss functions already implemented\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# nn package also has a variety of optimization algorithms implemented\n",
    "# we use the stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# e = 1.  # plotting purpose\n",
    "\n",
    "# Training\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass over the model to get the logits\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y)\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y == predicted).sum().float() / len(y)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # reset (zero) the gradients before running the backward pass over the model\n",
    "    # we need to do this because the gradients get accumulated at the same place across iterations\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params (weights and biases)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d44b1-0067-4d03-8333-958ebefbd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d2956-fe7b-43c3-a3ed-e00ea17ecd93",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We will now demonstrate how to build a linear model and a neural network model for a regression task using PyTorch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fe139-d7a4-4a85-8963-5a7c5da3b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "from plot_lib import plot_data, plot_model, set_default\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533739e-44ed-4a23-9b24-b20918ffeb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9c311-a503-492b-89a6-68d43929839b",
   "metadata": {},
   "source": [
    "### Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b3001-d101-4d80-bc21-2dca3aa47c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 1  # dimensions\n",
    "C = 1  # num_classes\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115c258-78e2-4d4e-b1c7-72b75a01c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1).to(device)\n",
    "y = X.pow(3) + 0.3 * torch.rand(X.size()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef9575-85dc-4a43-ac92-43f32daec528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes:\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ecf69-81ef-45b1-bf44-b74052441c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.cpu().numpy(), y.cpu().numpy())\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57888a-9b3d-405b-b749-333b69a16ab2",
   "metadata": {},
   "source": [
    "### Linear model\n",
    "\n",
    "Initialize the values of the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f1262-619a-4d17-88a6-a9d42816cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282ba83-72ba-47b8-99ea-12bf86827f12",
   "metadata": {},
   "source": [
    "Create the model and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754f70a-6433-4c2e-81e5-ec61c918f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "\n",
    "# print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d53f1e-a21c-4cc5-96a8-8025e79963a4",
   "metadata": {},
   "source": [
    "Create the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f06948-5fbf-4222-b371-d9442eb6429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use MSE (mean squared error) loss from the nn package for our regression task\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# we use the optim package to apply stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # forward pass over the model to get the logits (inputs to the loss function)\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss (MSE)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (t, loss.item()))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running the backward pass\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487090ea-a076-4e7b-8f2f-7e593b27bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plt.scatter(X.data.cpu().numpy(), y.data.cpu().numpy())\n",
    "plt.plot(X.data.cpu().numpy(), y_pred.data.cpu().numpy(), 'r-', lw=5)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485eaaa-d5a2-4a58-834a-ae6ff20b041e",
   "metadata": {},
   "source": [
    "Play around with the values of the hyper-parameters and the value of H (number of hidden units) to observe the change in the models being learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da58830-b949-4dbc-8e04-9a8fa9d406eb",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "We will test the following assumptions pertaining to CNNs \n",
    "\n",
    "* Compositionality obtained using many layers\n",
    "* Locality + stationarity of images assumed by the convolutional layers\n",
    "* Invariance of object class to translations assumed by the pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81153ea7-7bb9-4b93-9f5e-e6cb0b65ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from plot_lib import plot_data, plot_model, set_default\n",
    "\n",
    "set_default()\n",
    "# Get our device in a variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda70169-fafe-4b89-80d0-07bd03bc1bc4",
   "metadata": {},
   "source": [
    "### Load the Dataset (MNIST)\n",
    "\n",
    "Load the MNIST handwritten digits dataset. We can use the PyTorch DataLoader utilities for this. This will download, shuffle, normalize data and arrange it in batches. Normalizing involves subtracting some coefficient (usually the mean) from each pixel values and dividing the resulting pixel values by another coefficient (usually the variance of the original pixel values). We also display some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d1379-352a-44af-a31d-7fc668e3e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4420d6-4d90-4da9-a82b-e465d7813227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458776c6-8fd2-4924-9ceb-0dc0d1d7dc6a",
   "metadata": {},
   "source": [
    "### Create the model classes\n",
    "For comparison purposes we will create two models classes: \n",
    "1. Multi-layer Perceptron\n",
    "2. Convolutional Neural Network\n",
    "\n",
    "Pay special attention to the order of the layer while creating CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297610ad-a6ad-4182-8502-593f4036d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size, activation='relu'):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        if activation =='relu':\n",
    "            self.activation = nn.ReLU()  \n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else :\n",
    "            self.activation = nn.ReLU()\n",
    "            print(\"Activation function not implemented, using default ReLU\")\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            self.activation,\n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            self.activation,\n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)      \n",
    "        return self.network(x)\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size, activation='relu'):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_feature*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        if activation =='relu':\n",
    "            self.activation = nn.ReLU()  \n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else :\n",
    "            self.activation = nn.ReLU()\n",
    "            print(\"Activation function not implemented, using default ReLU\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x) # Will it make a difference if we apply the non-linearity after the pooling layer?\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*4*4) # this is where are flattening the 2D feature maps into a single 1D vector so as to be used by the subsequent fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce8b59-3d53-4865-ad24-0378ff67adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, perm=torch.arange(0, 784).long(), verbose=False):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send data to device, where the \"device\" is either a GPU if it exists or a CPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through the model\n",
    "        output = model(data)\n",
    "        # forward pass through the cross-entropy loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # backward pass through the cross-entropy loss function and the model\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            losses.append(loss.detach())\n",
    "            if verbose :\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return losses\n",
    "\n",
    "def test(model, perm=torch.arange(0, 784).long(), verbose=False):\n",
    "    model.eval()\n",
    "    accuracy_list = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # send data to device, where the \"device\" is either a GPU if it exists or a CPU\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # permute pixels\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy) \n",
    "        if verbose :\n",
    "            print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                accuracy))\n",
    "    return test_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475a29a-08e5-49a2-9e7c-4995d81ac36f",
   "metadata": {},
   "source": [
    "### Train a small MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64845ca6-98e1-48fa-a0b4-4c956eb6585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8 # number of hidden units\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn = model_fnn.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "    test(model_fnn, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f87a4c-3b32-4d9f-83be-12df9cf9070a",
   "metadata": {},
   "source": [
    "### Train a ConvNet with the same number of parameters\n",
    "Play around with the hyper-parameters to understand their relationship with model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed8fec-e3bb-4f6e-bc85-038d68774bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size, activation='relu')\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_cnn, optimizer, verbose=True)\n",
    "    test(model_cnn, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d47974-52c9-4b3b-914d-fd25eead20b3",
   "metadata": {},
   "source": [
    "### ConvNet performs better with the same number of parameters, thanks to its use of prior knowledge about images\n",
    "\n",
    "* Use of convolution: Locality and stationarity in images\n",
    "* Pooling: builds in some translation invariance\n",
    "\n",
    "### What happens if the assumptions are no longer true?\n",
    "Let us break the assumption of locality and permute the pixel within each image using an arbitrary permutation matrix. Also display the permuted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4be9b-d82b-4c4e-ac0c-a4fde570ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(784)\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i in range(10):\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    # permute pixels\n",
    "    image_perm = image.view(-1, 28*28).clone()\n",
    "    image_perm = image_perm[:, perm]\n",
    "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(image_perm.squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3385d-09b5-471a-933a-a269365ea595",
   "metadata": {},
   "source": [
    "### CNNs with permuted pixels\n",
    "What do you think will happen to CNNs when given permuted pixels as inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71140d21-ceaf-4a45-9390-35215adfbd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 3):\n",
    "    train(epoch, model_cnn, optimizer, perm, verbose=True)\n",
    "    test(model_cnn, perm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfe973-ea5a-41ae-9920-9851cc37600e",
   "metadata": {},
   "source": [
    "### MLPs with permuted pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c54aa-2796-4afb-a400-05a7a9ec164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8    # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, perm, verbose=True)\n",
    "    test(model_fnn, perm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199b5ea-bc7c-48f8-93cd-adfb2e27ec32",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "When we moved to Neural Networks we switched to ReLU as our activation function (or non-linearity). Recall that in logistic regression our non-linearty was the Sigmoid function. The sigmoid activation has a problem of vanishing gradients i.e. the gradients we get during backpropagation as often very close to 0. Think about what problem will this cause? In the next cell, we try both activations on our FC network and compare how the training proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbab997-cb0f-437e-aa19-fb894a1e3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8 # number of hidden units\n",
    "model_fnn_1 = FC2Layer(input_size, n_hidden, output_size, activation='relu')\n",
    "model_fnn_1 = model_fnn_1.to(device)\n",
    "model_fnn_2 = FC2Layer(input_size, n_hidden, output_size, activation='sigmoid')\n",
    "model_fnn_2 = model_fnn_2.to(device)\n",
    "\n",
    "optimizer_1 = optim.SGD(model_fnn_1.parameters(), lr=0.01, momentum=0.5)\n",
    "optimizer_2 = optim.SGD(model_fnn_2.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn_1)))\n",
    "train_losses_1 = []\n",
    "train_losses_2 = []\n",
    "for epoch in range(0, 15):\n",
    "    epoch_losses = train(epoch, model_fnn_1, optimizer_1)\n",
    "    train_losses_1.extend(epoch_losses)\n",
    "    epoch_losses = train(epoch, model_fnn_2, optimizer_2)\n",
    "    train_losses_2.extend(epoch_losses)\n",
    "plt.plot(train_losses_1, label='ReLU')\n",
    "plt.plot(train_losses_2, label='Sigmoid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455717b-923f-47e6-a92a-b8be5043e2e4",
   "metadata": {},
   "source": [
    "Think about why does the Sigmoid function have this issue but not the ReLU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e0f4d-0638-48f9-aa56-697ffc910e06",
   "metadata": {},
   "source": [
    "### Using Dropout to prevent overfitting\n",
    "Dropout is a regularization technique which is now widely used to train deep neural networks. The idea is to randomly dropout some activations while training the models (Why would this help?). While the effect of using dropout is demonstrable for models on larger datasets (which takes longer to train!), we use a toy regression problem to demonstrate how dropout prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309dc4f4-3b92-49ee-a42a-2493bb80c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50 #number of data points\n",
    "noise = 0.3\n",
    "\n",
    "#generate the train data\n",
    "X_train = torch.unsqueeze(torch.linspace(-1, 1, N),1)\n",
    "Y_train = X_train + noise * torch.normal(torch.zeros(N,1), torch.ones(N,1))\n",
    "\n",
    "#generate the test data\n",
    "X_test = torch.unsqueeze(torch.linspace(-1,1,N),1)\n",
    "Y_test = X_test + noise * torch.normal(torch.zeros(N,1), torch.ones(N,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049673a7-af84-49f4-8f88-52c7ce1213ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a neural network with out dropout\n",
    "N_h = 100 #hidden nodes\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(1, N_h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(N_h, N_h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(N_h, 1)\n",
    ")\n",
    "\n",
    "#create a network with dropout\n",
    "model_dropout = nn.Sequential(\n",
    "    nn.Linear(1, N_h),\n",
    "    nn.Dropout(0.5), #50 % probability \n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(N_h, N_h),\n",
    "    torch.nn.Dropout(0.2), #20% probability\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_h, 1),\n",
    ")\n",
    "\n",
    "plt.scatter(X_train.data.numpy(), Y_train.data.numpy(), c='purple', alpha=0.5, label='train')\n",
    "plt.scatter(X_test.data.numpy(), Y_test.data.numpy(), c='yellow', alpha=0.5, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbf6ac-2f5f-4ce0-bc06-cf6b9868ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of hidden layers\n",
    "N_h = 100\n",
    "\n",
    "# Define a FC model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_h, N_h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_h, 1),\n",
    ")\n",
    "\n",
    "# Define a FC model with dropout -> simply add nn.Dropout(p) where p is the probability of randomly dropping out a node \n",
    "model_dropout = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_h),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_h, N_h),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_h, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c80d86-f236-4391-9335-d9986121e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "opt_dropout = torch.optim.Adam(model_dropout.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2942de6-60de-45a0-9de7-c91c4b67d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1000\n",
    "\n",
    "# Training blueprint - we are not using minibatches since our data is small enough to use it completely\n",
    "for epoch in range(max_epochs):\n",
    "    pred = model(X_train) \n",
    "    loss = loss_fn(pred, Y_train)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    pred_dropout = model_dropout(X_train)\n",
    "    loss_dropout = loss_fn(pred_dropout, Y_train)\n",
    "    opt_dropout.zero_grad()\n",
    "    loss_dropout.backward()\n",
    "    opt_dropout.step()\n",
    "    \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        model_dropout.eval()\n",
    "        \n",
    "        test_pred = model(X_test)\n",
    "        test_loss = loss_fn(test_pred, Y_test)\n",
    "        \n",
    "        test_pred_dropout = model_dropout(X_test)\n",
    "        test_loss_dropout = loss_fn(test_pred_dropout, Y_test)\n",
    "        \n",
    "        plt.scatter(X_train.data.numpy(), Y_train.data.numpy(), c='purple', alpha=0.5, label='train')\n",
    "        plt.scatter(X_test.data.numpy(), Y_test.data.numpy(), c='yellow', alpha=0.5, label='test')\n",
    "        plt.plot(X_test.data.numpy(), test_pred.data.numpy(), 'r-', lw=3, label='normal')\n",
    "        plt.plot(X_test.data.numpy(), test_pred_dropout.data.numpy(), 'b--', lw=3,  label='dropout')\n",
    "        \n",
    "        plt.title('Epoch %d, Loss = %0.4f, Loss with dropout = %0.4f' % (epoch, test_loss, test_loss_dropout))\n",
    "        \n",
    "        plt.legend()\n",
    "\n",
    "        model.train()\n",
    "        model_dropout.train()\n",
    "        \n",
    "        plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1a0b5-9710-4c3b-8e1b-58b6a35ff1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
